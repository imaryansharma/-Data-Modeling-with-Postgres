%load_ext sql

%sql postgresql://student:student@127.0.0.1/sparkifydb

%sql SELECT * FROM songplays LIMIT 5;

%sql SELECT * FROM users LIMIT 5;

%sql SELECT * FROM songs LIMIT 5;

%sql SELECT * FROM artists LIMIT 5;

%sql SELECT * FROM time LIMIT 5;

REMEMBER: Restart this notebook to close connection to sparkifydb

Each time you run the cells above, remember to restart this notebook to close the connection to your database. Otherwise, you won't be able to run your code in create_tables.py, etl.py, or etl.ipynb files since you can't make multiple connections to the same database (in this case, sparkifydb).

import psycopg2

from sql_queries import create_table_queries, drop_table_queries

def create_database():

    """

    - Creates and connects to the sparkifydb

    - Returns the connection and cursor to sparkifydb

    """

    

    # connect to default database

    conn = psycopg2.connect("host=127.0.0.1 dbname=studentdb user=student password=student")

    conn.set_session(autocommit=True)

    cur = conn.cursor()

    

    # create sparkify database with UTF8 encoding

    cur.execute("DROP DATABASE IF EXISTS sparkifydb")

    cur.execute("CREATE DATABASE sparkifydb WITH ENCODING 'utf8' TEMPLATE template0")

​

    # close connection to default database

    conn.close()    

    

    # connect to sparkify database

    conn = psycopg2.connect("host=127.0.0.1 dbname=sparkifydb user=student password=student")

    cur = conn.cursor()

    

    return cur, conn

def drop_tables(cur, conn):

    """

    Drops each table using the queries in `drop_table_queries` list.

    """

    for query in drop_table_queries:

        cur.execute(query)

        conn.commit()

def create_tables(cur, conn):

    """

    Creates each table using the queries in `create_table_queries` list. 

    """

    for query in create_table_queries:

        cur.execute(query)

        conn.commit()

def main():

    """

    - Drops (if exists) and Creates the sparkify database. 

    

    - Establishes connection with the sparkify database and gets

    cursor to it.  

    

    - Drops all the tables.  

    

    - Creates all tables needed. 

    

    - Finally, closes the connection. 

    """

    cur, conn = create_database()

    

    drop_tables(cur, conn)

    create_tables(cur, conn)

​

    conn.close()

if __name__ == "__main__":

    main()

import os

import glob

import psycopg2

import pandas as pd

from sql_queries import *

def process_song_file(cur, filepath):

    """Reads songs log file row by row, selects needed fields and inserts them into song and artist tables.

        Parameters:

            cur (psycopg2.cursor()): Cursor of the sparkifydb database

            filepath (str): Filepath of the file to be analyzed

    """

    # open song file

    df = pd.read_json(filepath, lines=True)

​

    for value in df.values:

        artist_id, artist_latitude, artist_location, artist_longitude, artist_name, duration, num_songs, song_id, title, year = value

​

        # insert artist record

        artist_data = [artist_id, artist_name, artist_location, artist_longitude, artist_latitude]

        cur.execute(artist_table_insert, artist_data)

​

        # insert song record

        song_data = [song_id, title, artist_id, year, duration]

        cur.execute(song_table_insert, song_data)

def process_log_file(cur, filepath):

    """Reads user activity log file row by row, filters by NexSong, selects needed fields, transforms them and inserts

    them into time, user and songplay tables.

            Parameters:

                cur (psycopg2.cursor()): Cursor of the sparkifydb database

                filepath (str): Filepath of the file to be analyzed

    """

    # open log file

    df = pd.read_json(filepath, lines=True)

​

    # filter by NextSong action

    df = df[df['page']=='NextSong']

​

    # convert timestamp column to datetime

    t = pd.to_datetime(df['ts'], unit='ms') 

    

    # insert time data records

    time_data = []

    for line in t:

        time_data.append([line, line.hour, line.day, line.week, line.month, line.year, line.day_name()])

    column_labels = ('start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday')

    time_df = pd.DataFrame.from_records(time_data, columns=column_labels)

​

    for i, row in time_df.iterrows():

        cur.execute(time_table_insert, list(row))

​

    # load user table

    user_df = df[['userId', 'firstName', 'lastName', 'gender', 'level']]

​

    # insert user records

    for i, row in user_df.iterrows():

        cur.execute(user_table_insert, row)

​

    # insert songplay records

    for index, row in df.iterrows():

        

        # get songid and artistid from song and artist tables

        cur.execute(song_select, (row.song, row.artist, row.length))

        results = cur.fetchone()

        

        if results:

            songid, artistid = results

        else:

            songid, artistid = None, None

​

        # insert songplay record

        songplay_data = (index, pd.to_datetime(row.ts, unit='ms'), int(row.userId), row.level, songid, artistid, row.sessionId, row.location, row.userAgent)

        cur.execute(songplay_table_insert, songplay_data)

def process_data(cur, conn, filepath, func):

    """Walks through all files nested under filepath, and processes all logs found.

    Parameters:

        cur (psycopg2.cursor()): Cursor of the sparkifydb database

        conn (psycopg2.connect()): Connectio to the sparkifycdb database

        filepath (str): Filepath parent of the logs to be analyzed

        func (python function): Function to be used to process each log

    Returns:

        Name of files processed

    """

    # get all files matching extension from directory

    all_files = []

    for root, dirs, files in os.walk(filepath):

        files = glob.glob(os.path.join(root,'*.json'))

        for f in files :

            all_files.append(os.path.abspath(f))

​

    # get total number of files found

    num_files = len(all_files)

    print('{} files found in {}'.format(num_files, filepath))

​

    # iterate over files and process

    for i, datafile in enumerate(all_files, 1):

        func(cur, datafile)

        conn.commit()

        print('{}/{} files processed.'.format(i, num_files))

    

    return all_files

def main():

    """Function used to extract, transform all data from song and user activity logs and load it into a PostgreSQL DB

        Usage: python etl.py

    """

    conn = psycopg2.connect("host=127.0.0.1 dbname=sparkifydb user=student password=student")

    cur = conn.cursor()

​

    process_data(cur, conn, filepath='data/song_data', func=process_song_file)

    process_data(cur, conn, filepath='data/log_data', func=process_log_file)

​

    conn.close()

if __name__ == "__main__":

    main()

71 files found in data/song_data
1/71 files processed.
2/71 files processed.
3/71 files processed.
4/71 files processed.
5/71 files processed.
6/71 files processed.
7/71 files processed.
8/71 files processed.
9/71 files processed.
10/71 files processed.
11/71 files processed.
12/71 files processed.
13/71 files processed.
14/71 files processed.
15/71 files processed.
16/71 files processed.
17/71 files processed.
18/71 files processed.
19/71 files processed.
20/71 files processed.
21/71 files processed.
22/71 files processed.
23/71 files processed.
24/71 files processed.
25/71 files processed.
26/71 files processed.
27/71 files processed.
28/71 files processed.
29/71 files processed.
30/71 files processed.
31/71 files processed.
32/71 files processed.
33/71 files processed.
34/71 files processed.
35/71 files processed.
36/71 files processed.
37/71 files processed.
38/71 files processed.
39/71 files processed.
40/71 files processed.
41/71 files processed.
42/71 files processed.
43/71 files processed.
44/71 files processed.
45/71 files processed.
46/71 files processed.
47/71 files processed.
48/71 files processed.
49/71 files processed.
50/71 files processed.
51/71 files processed.
52/71 files processed.
53/71 files processed.
54/71 files processed.
55/71 files processed.
56/71 files processed.
57/71 files processed.
58/71 files processed.
59/71 files processed.
60/71 files processed.
61/71 files processed.
62/71 files processed.
63/71 files processed.
64/71 files processed.
65/71 files processed.
66/71 files processed.
67/71 files processed.
68/71 files processed.
69/71 files processed.
70/71 files processed.
71/71 files processed.
30 files found in data/log_data
1/30 files processed.
2/30 files processed.
3/30 files processed.
4/30 files processed.
5/30 files processed.
6/30 files processed.
7/30 files processed.
8/30 files processed.
9/30 files processed.
10/30 files processed.
11/30 files processed.
12/30 files processed.
13/30 files processed.
14/30 files processed.
15/30 files processed.
16/30 files processed.
17/30 files processed.
18/30 files processed.
19/30 files processed.
20/30 files processed.
21/30 files processed.
22/30 files processed.
23/30 files processed.
24/30 files processed.
25/30 files processed.
26/30 files processed.
27/30 files processed.
28/30 files processed.
29/30 files processed.
30/30 files processed.

import os

import glob

import psycopg2

import pandas as pd

from sql_queries import *

conn = psycopg2.connect("host=127.0.0.1 dbname=sparkifydb user=student password=student")

cur = conn.cursor()

def get_files(filepath):

    all_files = []

    for root, dirs, files in os.walk(filepath):

        files = glob.glob(os.path.join(root,'*.json'))

        for f in files :

            all_files.append(os.path.abspath(f))

    

    return all_files

song_files = get_files('data/song_data/')

filepath = song_files[0]

print(filepath)

/home/workspace/data/song_data/A/B/C/TRABCRU128F423F449.json

df = pd.read_json(filepath, lines=True).iloc[0]

df['year'] = df['year'].item()

df.head()

artist_id           AR8IEZO1187B99055E
artist_latitude                    NaN
artist_location                       
artist_longitude                   NaN
artist_name               Marc Shaiman
Name: 0, dtype: object

song_data = df[['song_id', 'title', 'artist_id', 'year', 'duration']].values.tolist()

song_data

['SOINLJW12A8C13314C',
 'City Slickers',
 'AR8IEZO1187B99055E',
 2008,
 149.86403999999999]

cur.execute(song_table_insert, song_data)

conn.commit()

artist_data = df[['artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude']].values.tolist()

artist_data

['AR8IEZO1187B99055E', 'Marc Shaiman', '', nan, nan]

cur.execute(artist_table_insert, artist_data)

conn.commit()

log_files = get_files('data/log_data')

filepath = log_files[0]

print(filepath)

/home/workspace/data/log_data/2018/11/2018-11-29-events.json

df = pd.read_json(filepath, lines=True)

df.head()

	artist 	auth 	firstName 	gender 	itemInSession 	lastName 	length 	level 	location 	method 	page 	registration 	sessionId 	song 	status 	ts 	userAgent 	userId
0 	Sydney Youngblood 	Logged In 	Jacob 	M 	53 	Klein 	238.07955 	paid 	Tampa-St. Petersburg-Clearwater, FL 	PUT 	NextSong 	1.540558e+12 	954 	Ain't No Sunshine 	200 	1543449657796 	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 	73
1 	Gang Starr 	Logged In 	Layla 	F 	88 	Griffin 	151.92771 	paid 	Lake Havasu City-Kingman, AZ 	PUT 	NextSong 	1.541057e+12 	984 	My Advice 2 You (Explicit) 	200 	1543449690796 	"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... 	24
2 	3OH!3 	Logged In 	Layla 	F 	89 	Griffin 	192.52200 	paid 	Lake Havasu City-Kingman, AZ 	PUT 	NextSong 	1.541057e+12 	984 	My First Kiss (Feat. Ke$ha) [Album Version] 	200 	1543449841796 	"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... 	24
3 	RÃÂ¶yksopp 	Logged In 	Jacob 	M 	54 	Klein 	369.81506 	paid 	Tampa-St. Petersburg-Clearwater, FL 	PUT 	NextSong 	1.540558e+12 	954 	The Girl and The Robot 	200 	1543449895796 	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 	73
4 	Kajagoogoo 	Logged In 	Layla 	F 	90 	Griffin 	223.55546 	paid 	Lake Havasu City-Kingman, AZ 	PUT 	NextSong 	1.541057e+12 	984 	Too Shy 	200 	1543450033796 	"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... 	24

df = df[df['page']=='NextSong']

df.head()

	artist 	auth 	firstName 	gender 	itemInSession 	lastName 	length 	level 	location 	method 	page 	registration 	sessionId 	song 	status 	ts 	userAgent 	userId
0 	Sydney Youngblood 	Logged In 	Jacob 	M 	53 	Klein 	238.07955 	paid 	Tampa-St. Petersburg-Clearwater, FL 	PUT 	NextSong 	1.540558e+12 	954 	Ain't No Sunshine 	200 	1543449657796 	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 	73
1 	Gang Starr 	Logged In 	Layla 	F 	88 	Griffin 	151.92771 	paid 	Lake Havasu City-Kingman, AZ 	PUT 	NextSong 	1.541057e+12 	984 	My Advice 2 You (Explicit) 	200 	1543449690796 	"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... 	24
2 	3OH!3 	Logged In 	Layla 	F 	89 	Griffin 	192.52200 	paid 	Lake Havasu City-Kingman, AZ 	PUT 	NextSong 	1.541057e+12 	984 	My First Kiss (Feat. Ke$ha) [Album Version] 	200 	1543449841796 	"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... 	24
3 	RÃÂ¶yksopp 	Logged In 	Jacob 	M 	54 	Klein 	369.81506 	paid 	Tampa-St. Petersburg-Clearwater, FL 	PUT 	NextSong 	1.540558e+12 	954 	The Girl and The Robot 	200 	1543449895796 	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4... 	73
4 	Kajagoogoo 	Logged In 	Layla 	F 	90 	Griffin 	223.55546 	paid 	Lake Havasu City-Kingman, AZ 	PUT 	NextSong 	1.541057e+12 	984 	Too Shy 	200 	1543450033796 	"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebK... 	24

t = pd.to_datetime(df['ts'], unit='ms') 

t.head()

0   2018-11-29 00:00:57.796
1   2018-11-29 00:01:30.796
2   2018-11-29 00:04:01.796
3   2018-11-29 00:04:55.796
4   2018-11-29 00:07:13.796
Name: ts, dtype: datetime64[ns]

time_data = []

for line in t:

    time_data.append([line, line.hour, line.day, line.week, line.month, line.year, line.day_name()])

column_labels = ('start_time', 'hour', 'day', 'week', 'month', 'year', 'weekday')

time_df = pd.DataFrame.from_records(time_data, columns=column_labels)

time_df.head()

	start_time 	hour 	day 	week 	month 	year 	weekday
0 	2018-11-29 00:00:57.796 	0 	29 	48 	11 	2018 	Thursday
1 	2018-11-29 00:01:30.796 	0 	29 	48 	11 	2018 	Thursday
2 	2018-11-29 00:04:01.796 	0 	29 	48 	11 	2018 	Thursday
3 	2018-11-29 00:04:55.796 	0 	29 	48 	11 	2018 	Thursday
4 	2018-11-29 00:07:13.796 	0 	29 	48 	11 	2018 	Thursday

for i, row in time_df.iterrows():

    cur.execute(time_table_insert, list(row))

    conn.commit()

user_df = df[['userId', 'firstName', 'lastName', 'gender', 'level']]

user_df.head()

	userId 	firstName 	lastName 	gender 	level
0 	73 	Jacob 	Klein 	M 	paid
1 	24 	Layla 	Griffin 	F 	paid
2 	24 	Layla 	Griffin 	F 	paid
3 	73 	Jacob 	Klein 	M 	paid
4 	24 	Layla 	Griffin 	F 	paid

for i, row in user_df.iterrows():

    cur.execute(user_table_insert, row)

    conn.commit()

for index, row in df.iterrows():

​

    # get songid and artistid from song and artist tables

    cur.execute(song_select, (row.song, row.artist, row.length))

    results = cur.fetchone()

    

    if results:

        songid, artistid = results

    else:

        songid, artistid = None, None

​

    # insert songplay record

    songplay_data = (index, pd.to_datetime(row.ts, unit='ms'), int(row.userId), row.level, songid, artistid, row.sessionId, row.location, row.userAgent)

    cur.execute(songplay_table_insert, songplay_data)

    conn.commit()

conn.close()
